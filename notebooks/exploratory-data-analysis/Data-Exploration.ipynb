{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for Yelp Dataset\n",
    "\n",
    "This is a notebook that explores the sample Yelp dataset provided by Kaggle (https://www.kaggle.com/yelp-dataset/yelp-dataset).\n",
    " \n",
    "The datasets are as follows:\n",
    "- **yelp_academic_dataset_business.json**: TBD\n",
    "- **yelp_academic_dataset_checkin.json**: TBD\n",
    "- **yelp_academic_dataset_review.json**: TBD\n",
    "- **yelp_academic_dataset_tip.json**: TBD\n",
    "- **yelp_academic_dataset_user.json**: TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install findspark py4j ipywidgets matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/ana/server/spark-3.0.1-bin-hadoop2.7\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#Seaborn for data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"data-exploration-yelp\")\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business = \"./yelp-raw/yelp_academic_dataset_business.json\"\n",
    "\n",
    "yelp_checkin = \"./yelp-raw/yelp_academic_dataset_checkin.json\"\n",
    "\n",
    "yelp_review= \"./yelp-raw/yelp_academic_dataset_review.json\"\n",
    "    \n",
    "yelp_tip= \"./yelp-raw/yelp_academic_dataset_tip.json\"\n",
    "\n",
    "yelp_user= \"./yelp-raw/yelp_academic_dataset_user.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "\n",
    "We are going to explore each dataset and understand various statistical analysis of the data.\n",
    "\n",
    "#### Business Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = spark.read.json(yelp_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattends dataframe to remove nesting.\n",
    "flat_business_df = business_df.select(\"address\", \"attributes.*\", \"business_id\", \"categories\", \"city\", \"hours.*\", \n",
    "                                      \"is_open\", \"latitude\", \"longitude\", \"name\", \"postal_code\", \"review_count\", \n",
    "                                      \"stars\", \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_business_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_business_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_business_df.write.parquet(\"./yelp_transformed/business_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_business_df = spark.read.parquet(\"./yelp_transformed/business_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_business_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_business_df.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of all the rows to get the amount of business.\n",
    "total_businesses = transformed_business_df.count()\n",
    "total_businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates new dataframe that counts all nulls within a column iteratively.\n",
    "null_df = transformed_business_df.select([count(when(col(c).isNull(), c)).alias(c) for c in transformed_business_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_pd = null_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide all null column counts by the amount of businesses, to get total percentage of missing values\n",
    "null_percentage_pd = null_pd.div(total_businesses / 100, axis=0)\n",
    "null_percentage_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter null percentages dataframe by set percentage. Here we default by 50%, as we want to try our first pass \n",
    "# of training with relatively stable columns.\n",
    "\n",
    "percentage_to_filter_by = 50\n",
    "\n",
    "null_filtered_pd = null_percentage_pd[null_percentage_pd < percentage_to_filter_by]\n",
    "null_filtered_na = null_filtered_pd.dropna(axis=1, how='all')\n",
    "null_filtered_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out weekday opening time columns, as they are not going to be very useful for model recommendations.\n",
    "\n",
    "final_null_pd = null_filtered_na.drop([\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"], axis=1)\n",
    "final_null_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = final_null_pd.columns.tolist()\n",
    "columns_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_business_df = transformed_business_df.select(*columns_to_keep)\n",
    "filtered_business_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_columns = widgets.Dropdown(options = filtered_business_df.columns)\n",
    "\n",
    "output_column = widgets.Output()\n",
    "\n",
    "def dropdown_columns_event_handler(change):\n",
    "    output_column.clear_output()\n",
    "    \n",
    "    filtered_business_df.groupBy(change.new).count().sort(col(\"count\").desc()).show(50, False)\n",
    "\n",
    "dropdown_columns.observe(dropdown_columns_event_handler, names='value')\n",
    "display(dropdown_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_count_df = filtered_business_df.groupBy(col(\"stars\")).count().sort(col(\"count\").desc())\n",
    "stars_count_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_pd = stars_count_df.toPandas()\n",
    "stars_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"stars\", y=\"count\", kind=\"bar\", data=stars_pd)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
